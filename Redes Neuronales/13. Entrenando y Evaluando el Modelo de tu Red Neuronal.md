# 13. Entrenando y Evaluando el Modelo de tu Red Neuronal
En este capítulo se enseñará cómo entrenar y evaluar el modelo de una red neuronal utilizando los datos de entrenamiento y prueba. Se explicará cómo entrenar el modelo con los datos de entrenamiento, y evaluar la precisión del modelo utilizando los datos de prueba.

## 13.1. Entrenamiento del modelo con los datos de entrenamiento

El entrenamiento es donde alimentamos nuestros datos de entrenamiento al modelo. En este proceso, el modelo intenta aprender patrones útiles en los datos de entrenamiento que le permitirán realizar la tarea en cuestión, como la clasificación de imágenes.

### Conceptos clave

#### Hiperparámetros, Función de pérdida y Optimizador

Los hiperparámetros son los parámetros que definimos antes de entrenar el modelo. Estos incluyen el número de épocas (iteraciones completas a través de los datos de entrenamiento), la tasa de aprendizaje (qué tan rápido el modelo aprende), el tamaño del lote (número de muestras de entrenamiento utilizadas en una iteración), entre otros.

La función de pérdida es una forma de medir cuán bien se desempeña el modelo en los datos de entrenamiento. Deseamos minimizar esta función para "guiar" el modelo en la dirección correcta.

El optimizador es el algoritmo que utilizamos para minimizar la función de pérdida. Este algoritmo realiza pequeños ajustes a los pesos y sesgos de la red para mejorar su rendimiento.

**Ejemplo:** Puedes imaginar el entrenamiento del modelo como el proceso de aprender a lanzar dardos. Al principio, tus lanzamientos pueden no acertar en el objetivo (alta función de pérdida). Sin embargo, con práctica y ajuste, puedes mejorar tu puntería (minimizar la función de pérdida con un optimizador).

### Ajuste de los hiperparámetros

Los hiperparámetros son parámetros ajustables que determinan la estrategia de aprendizaje del algoritmo. Por ejemplo, durante el entrenamiento de una red neuronal, podríamos ajustar hiperparámetros como el número de capas en la red, el número de nodos en cada capa, el tipo de optimizador, la tasa de aprendizaje del optimizador, y otros.

Ajustar estos hiperparámetros puede tener un gran impacto en el rendimiento de la red neuronal. Sin embargo, encontrar la combinación correcta de hiperparámetros puede ser un desafío. A menudo, los ingenieros de aprendizaje automático realizarán una "búsqueda de cuadrícula" o una "búsqueda aleatoria" para encontrar la mejor combinación de hiperparámetros.

Entonces, procedamos a entrenar nuestro modelo y ajustar los hiperparámetros utilizando el siguiente código:

```python
from keras.optimizers import RMSprop

# Definimos los hiperparámetros
epochs = 10
batch_size = 128
learning_rate = 0.01

# Configuramos el optimizador con la tasa de aprendizaje
optimizer = RMSprop(learning_rate=learning_rate)

# Definimos el optimizador, la función de pérdida y las métricas que queremos rastrear
model.compile(optimizer=optimizer, 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Entrenamos el modelo
history = model.fit(train_data, train_labels, 
                    epochs=epochs, 
                    batch_size=batch_size)
```

En este ejemplo, hemos definido el número de épocas, el tamaño del lote y la tasa de aprendizaje como nuestros hiperparámetros. Ajustamos la tasa de aprendizaje del optimizador antes de pasarla a la función `compile`. Luego entrenamos el modelo con el método `fit` y guardamos la historia de entrenamiento para futuras referencias.

## 13.2. Evaluación de la precisión del modelo con los datos de prueba

La evaluación del modelo nos permite medir el rendimiento de nuestro modelo entrenado en un conjunto de datos independiente, conocido como datos de prueba. Durante esta fase, hacemos predicciones con nuestro modelo y luego las comparamos con las etiquetas reales para determinar la precisión de nuestras predicciones. 

Además, utilizamos diversas métricas de evaluación para obtener una comprensión más detallada del rendimiento de nuestro modelo.

### Predicciones del modelo
Para realizar predicciones con nuestro modelo, usamos el método `predict()` del modelo. Este método toma una entrada (en nuestro caso, los datos de prueba) y produce una salida (las predicciones).

```python
# Realizando predicciones
predictions = model.predict(test_data)
```

Cada entrada en 'predictions' es un vector de longitud 10 (asumiendo que estamos clasificando imágenes en 10 clases). El índice con el valor más alto es la clase predicha por el modelo.

### Comparando con las etiquetas reales
Para ver cuán bien se desempeña nuestro modelo, debemos comparar nuestras predicciones con las etiquetas reales. Esto se puede hacer manualmente, o podemos usar la función `argmax()` de NumPy para obtener los índices de las clases predichas y luego compararlos con las etiquetas reales.

```python
import numpy as np

# Convertir las predicciones a etiquetas
predicted_labels = np.argmax(predictions, axis=1)

# Convertir las etiquetas de prueba de one-hot a clases
test_labels_classes = np.argmax(test_labels, axis=1)

# Comparando con las etiquetas reales
accuracy = np.sum(predicted_labels == test_labels_classes) / len(test_labels_classes)
print('Test accuracy:', accuracy)
```

### Métricas de evaluación
Además de la precisión, que simplemente cuenta el porcentaje de clasificaciones correctas, hay otras métricas de evaluación que pueden proporcionarnos más información sobre el rendimiento del modelo. Estas incluyen la precisión (qué proporción de identificaciones positivas fue realmente correcta), el recall (qué proporción de positivos reales se identificó correctamente), la puntuación F1 (una combinación de precisión y recall) y la curva ROC (una representación gráfica del rendimiento del clasificador).

```python
from sklearn.metrics import classification_report

# Convertir las etiquetas de prueba de one-hot a clases
test_labels_classes = np.argmax(test_labels, axis=1)

# Imprimir el informe de clasificación
print(classification_report(test_labels_classes, predicted_labels))
```

Este informe de clasificación nos dará una visión detallada de cómo se desempeña nuestro modelo en cada clase individualmente, lo que puede ser útil para identificar clases problemáticas que podrían beneficiarse de más datos de entrenamiento o de un ajuste de los hiperparámetros.

### Evaluando el modelo
La evaluación del modelo es un paso fundamental después de entrenar nuestro modelo. Para ello, usamos el método `evaluate()` del modelo. Esta función devolverá la pérdida del modelo y las métricas de evaluación calculadas para el conjunto de datos de prueba.

```python
# Evaluación del modelo
loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)
print('Test loss:', loss)
print('Test accuracy:', accuracy)
```

En este caso, la pérdida (loss) nos da una medida de cuán incorrectas fueron las predicciones del modelo en promedio, mientras que la precisión (accuracy) nos indica la proporción de imágenes que fueron clasificadas correctamente. Asegúrate de tener en cuenta tanto la pérdida como la precisión al evaluar el rendimiento de tu modelo.