# 6. Arquitectura de una Red Neuronal
En este capítulo se explorará la arquitectura general de una red neuronal. Se discutirán los conceptos de capa de entrada, capas ocultas y capa de salida, así como las características generales y específicas de cada una. Se explicará cómo las redes neuronales están compuestas por matrices y vectores, y cómo se realiza el producto matricial y se aplica el sesgo (bias).

## 6.1. Capa de entrada, capas ocultas y capa de salida

En este apartado, nos adentraremos en la arquitectura general de una red neuronal y exploraremos cómo se conectan las capas de una red neuronal, así como el procesamiento de la información a medida que los datos fluyen a través de la red.

### Arquitectura de una red neuronal
Una red neuronal está compuesta por varias capas de neuronas interconectadas. La información fluye desde la capa de entrada a través de las capas ocultas hasta llegar a la capa de salida. Cada capa está formada por neuronas, que son unidades computacionales que reciben entradas, realizan cálculos y generan salidas.

### Capa de entrada
La capa de entrada es la primera capa de una red neuronal y tiene la tarea de recibir los datos iniciales que serán procesados. Cada neurona en la capa de entrada representa una característica o variable del conjunto de datos de entrada. Esta capa proporciona la forma en la que la información externa se introduce en la red neuronal.

En el contexto del reconocimiento de imágenes, consideremos una red neuronal para identificar si una imagen contiene un gato o un perro. La capa de entrada podría estar compuesta por neuronas que representan los valores de los píxeles de la imagen. Cada neurona en esta capa recibiría el valor correspondiente al nivel de gris o color de un píxel específico. Por ejemplo, si la imagen es de 64x64 píxeles, tendríamos 4,096 neuronas de entrada, donde cada neurona representaría el valor de un píxel.

**Ejemplo:** Supongamos que tenemos una imagen en escala de grises de un gato. Cada neurona en la capa de entrada representa el nivel de gris de un píxel. Si la imagen se representa como una matriz de 64x64 píxeles, la primera neurona puede representar el nivel de gris del píxel en la posición (0, 0), la segunda neurona el nivel de gris del píxel en la posición (0, 1), y así sucesivamente.

### Capas ocultas
Las capas ocultas son las capas intermedias entre la capa de entrada y la capa de salida de una red neuronal. Estas capas realizan el procesamiento y la transformación de los datos a medida que fluyen a través de la red. Cada capa oculta está compuesta por un conjunto de neuronas interconectadas, y su número y disposición dependen de la arquitectura de la red neuronal.

En las capas ocultas, las neuronas reciben las entradas de las neuronas en la capa anterior y realizan cálculos utilizando los pesos asociados a esas conexiones. Luego, se aplica una función de activación a la suma ponderada de las entradas y los pesos para generar la salida de cada neurona. El propósito de estas capas es extraer características y patrones más complejos y abstractos a medida que se profundiza en la estructura de la red.

**Ejemplo:** Siguiendo con el ejemplo del reconocimiento de gatos y perros, supongamos que tenemos una red neuronal con una capa oculta. Cada neurona en la capa oculta recibiría conexiones de las neuronas en la capa de entrada. Utilizando los pesos asociados a esas conexiones, cada neurona en la capa oculta realizaría una combinación lineal de las entradas ponderadas. Luego, se aplicaría una función de activación no lineal, como la función ReLU (Rectified Linear Unit), para generar la salida de cada neurona oculta. Estas salidas se propagarían hacia la capa de salida.

### Capa de salida
La capa de salida es la última capa de una red neuronal y produce los resultados finales o predicciones basadas en la información procesada en las capas anteriores. El número de neuronas en la capa de salida depende del tipo de problema que se esté abordando. En el caso de un problema de clasificación binaria, tendríamos una neurona de salida que representa la probabilidad de pertenecer a una clase específica. En un problema de clasificación multiclase, habría una neurona por cada clase posible.

Cada neurona en la capa de salida representa una clase o categoría diferente, y su salida proporciona una medida de la probabilidad o la activación de la clase correspondiente. Para convertir las salidas de las neuronas en una predicción final, se suele aplicar una función de activación, como la función sigmoide para problemas de clasificación binaria o la función softmax para problemas de clasificación multiclase.

**Ejemplo:** En el ejemplo del reconocimiento de gatos y perros, si estamos clasificando entre estas dos clases, la capa de salida tendría una neurona. La salida de esta neurona representaría la probabilidad de que la imagen sea un gato. Si la salida es cercana a 1, indicaría una alta probabilidad de ser un gato, mientras que si es cercana a 0, indicaría una alta probabilidad de ser un perro.

## 6.2. Matrices y vectores en las redes neuronales

En este apartado, exploraremos cómo las redes neuronales utilizan matrices y vectores en sus operaciones. Estos conceptos matemáticos son fundamentales para el procesamiento de la información en una red.

## Matrices en las redes neuronales

En el contexto de las redes neuronales, las matrices son estructuras de datos bidimensionales que se utilizan para representar los pesos y las conexiones entre las neuronas de diferentes capas. Cada valor en una matriz representa el peso de la conexión entre dos neuronas.

**Ejemplo:** Imagina que tienes una red neuronal con una capa de entrada que tiene 3 neuronas y una capa oculta con 4 neuronas. Puedes representar las conexiones entre estas capas utilizando una matriz de 4 filas y 3 columnas. Cada valor en la matriz será el peso de la conexión entre una neurona de la capa de entrada y una neurona de la capa oculta.

```
Matriz de conexiones entre capa de entrada y capa oculta:

|   | Neurona 1 | Neurona 2 | Neurona 3 |
|---|-----------|-----------|-----------|
| 1 |   0.5     |   0.2     |   0.8     |
| 2 |   0.1     |   0.6     |   0.4     |
| 3 |   0.9     |   0.3     |   0.7     |
| 4 |   0.2     |   0.5     |   0.1     |
```

En este ejemplo, cada fila de la matriz representa una neurona en la capa oculta, y cada columna representa una neurona en la capa de entrada. Los valores en la matriz representan los pesos de las conexiones entre las neuronas.

**Ejemplo:** Imagina que estás construyendo un sistema de clasificación de frutas utilizando una red neuronal. La capa de entrada de la red neuronal tiene 3 neuronas que representan las características de peso, tamaño y color de las frutas. La capa oculta tiene 4 neuronas, y cada una de ellas tiene un objetivo específico de clasificación basado en diferentes combinaciones de características.

*Neurona 1 de la capa oculta:* Esta neurona tiene como objetivo clasificar frutas en función del peso y el tamaño. Los pesos de las conexiones entre la capa de entrada y esta neurona representan la importancia relativa del peso y el tamaño en la clasificación de frutas.

*Neurona 2 de la capa oculta:* Esta neurona tiene como objetivo clasificar frutas en función del tamaño y el color. Los pesos de las conexiones entre la capa de entrada y esta neurona representan la importancia relativa del tamaño y el color en la clasificación de frutas.

*Neurona 3 de la capa oculta:* Esta neurona tiene como objetivo clasificar frutas en función del peso y el color. Los pesos de las conexiones entre la capa de entrada y esta neurona representan la importancia relativa del peso y el color en la clasificación de frutas.

*Neurona 4 de la capa oculta:* Esta neurona tiene como objetivo clasificar frutas en función de todas las características: peso, tamaño y color. Los pesos de las conexiones entre la capa de entrada y esta neurona representan la importancia relativa de todas las características en la clasificación de frutas.

Si por ejemplo tenemos una manzana con un peso de 150 gramos, un tamaño medio y un color rojo, la red neuronal realizará cálculos utilizando los pesos correspondientes y las características de la manzana. Cada neurona en la capa oculta calculará una activación basada en su objetivo de clasificación. Luego, la red neuronal utilizará estas activaciones para realizar una clasificación final y determinar si la fruta es una manzana o no.

## Vectores en las redes neuronales

En las redes neuronales, los vectores se utilizan para representar las activaciones de las neuronas en cada capa. Un vector es una estructura de datos unidimensional que contiene una secuencia de valores.

**Ejemplo:** Supongamos que tienes una red neuronal con una capa oculta que tiene 4 neuronas. Puedes representar las activaciones de estas neuronas utilizando un vector de longitud 4.

```
Vector de activaciones en la capa oculta: [0.8, 0.2, 0.6, 0.4]
```

En este ejemplo, cada valor en el vector representa la activación de una neurona en la capa oculta. Estos valores pueden interpretarse como la "intensidad" o el "nivel de activación" de cada neurona.

**Ejemplo:** Imagina que estás construyendo un sistema de recomendación de películas basado en las preferencias de los usuarios. Puedes utilizar una red neuronal para calcular la afinidad de un usuario por diferentes géneros de películas. La capa oculta de la red neuronal podría tener 4 neuronas, cada una representando un género de película (por ejemplo, acción, comedia, drama, ciencia ficción). Las activaciones de estas neuronas se podrían representar utilizando un vector de longitud 4, donde cada valor en el vector representa la afinidad del usuario por un género específico.

## Producto matricial en las redes neuronales

El producto matricial es una operación clave en las redes neuronales, ya que permite combinar las entradas con los pesos de las conexiones entre las neuronas. Esta operación se utiliza en el proceso de propagación hacia adelante (forward propagation) para calcular las activaciones de las neuronas en las capas ocultas y de salida.

**Ejemplo:**
Supongamos que tienes una red neuronal con una capa de entrada de 3 neuronas y una capa oculta de 4 neuronas. Además, tienes una matriz de conexiones entre estas capas, representada por la siguiente matriz:

```
Matriz de conexiones entre capa de entrada y capa oculta:

|   | Neurona 1 | Neurona 2 | Neurona 3 |
|---|-----------|-----------|-----------|
| 1 |   0.5     |   0.2     |   0.8     |
| 2 |   0.1     |   0.6     |   0.4     |
| 3 |   0.9     |   0.3     |   0.7     |
| 4 |   0.2     |   0.5     |   0.1     |
```

Y supongamos que las activaciones en la capa de entrada son [1, 2, 3]. Para calcular las activaciones en la capa oculta, realizamos el producto matricial entre el vector de activaciones de la capa de entrada y la matriz de conexiones:

```
Producto matricial entre vector de activaciones y matriz de conexiones:

[1, 2, 3] * 
|   | Neurona 1 | Neurona 2 | Neurona 3 |
|---|-----------|-----------|-----------|
| 1 |   0.5     |   0.2     |   0.8     |
| 2 |   0.1     |   0.6     |   0.4     |
| 3 |   0.9     |   0.3     |   0.7     |
| 4 |   0.2     |   0.5     |   0.1     | 

= [1*0.5 + 2*0.2 + 3*0.8, 1*0.1 + 2*0.6 + 3*0.4, 1*0.9 + 2*0.3 + 3*0.7, 1*0.2 + 2*0.5 + 3*0.1]
= [3.3, 2.5, 3.6, 1.5]
```

En este ejemplo, el producto matricial nos da como resultado un vector de activaciones para la capa oculta. Cada valor en el vector representa la activación de una neurona en la capa oculta, calculada mediante la combinación lineal de las entradas con los pesos de las conexiones.

**Ejemplo:** Imagina que tienes un conjunto de datos que representa las características de diferentes autos, como la potencia del motor, el consumo de combustible y la capacidad de carga. Puedes utilizar una red neuronal para predecir el precio de un auto en función de estas características. La capa de entrada de la red neuronal podría tener 3 neuronas, una para cada característica. Si tienes una matriz de conexiones entre la capa de entrada y la capa oculta, puedes realizar el producto matricial entre el vector de características de un auto y la matriz de conexiones para obtener las activaciones en la capa oculta. Esto permitirá que la red neuronal realice cálculos y predicciones basados en las características del auto.

## Aplicación del sesgo (bias) en las redes neuronales

El sesgo, también conocido como bias, es un término adicional utilizado en las redes neuronales para introducir una componente constante en la activación de una neurona. Añadir un sesgo permite ajustar la salida de una neurona incluso cuando todas sus entradas son cero.

**Ejemplo:** Supongamos que tienes una red neuronal con una capa oculta de 4 neuronas y un vector de activaciones en esta capa [0.8, 0.2, 0.6, 0.4]. Además, tienes un vector de sesgos para estas neuronas [0.1, 0.2, 0.3, 0.4]. Para aplicar el sesgo, simplemente sumamos el valor del sesgo a la activación de cada neurona:

```
Vector de activaciones en la capa oculta: [0.8, 0.2, 0.6, 0.4]
Vector de sesgos: [0.1, 0.2, 0.3, 0.4]

Aplicación del sesgo:

[0.8 + 0.1, 0.2 + 0.2, 0.6 + 0.3, 0.4 + 0.4]
= [0.9, 0.4, 0.9, 0.8]
```

En este ejemplo, el sesgo se suma a cada valor en el vector de activaciones de la capa oculta, lo que ajusta la salida de las neuronas incluso cuando todas las entradas son cero.

**Ejemplo:** Imagina que estás construyendo un sistema de reconocimiento facial utilizando una red neuronal. Después de aplicar una serie de operaciones y cálculos, obtienes un vector de activaciones que representa las características faciales detectadas. Sin embargo, para asegurar una detección precisa, es importante tener un sesgo para cada neurona en la capa final de reconocimiento. Esto permitirá que la red neuronal haga ajustes finos y tome decisiones incluso cuando las características detectadas sean sutiles o no estén presentes en todas las imágenes.