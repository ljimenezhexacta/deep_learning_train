## 8. Funciones de Activación en las Redes Neuronales
Aquí se estudiarán las funciones de activación utilizadas en las redes neuronales. Se presentarán diferentes tipos de funciones, incluyendo las discretas y las continuas, como la función escalón, la función signo, la función sigmoidal, la función tangente hiperbólica, la función lineal rectificada (ReLU) y la función softmax. Se explicará el propósito y la aplicabilidad de cada función en el contexto de una red neuronal.

## 8.1. ¿Qué son las funciones de activación en las redes neuronales?

En el contexto de las redes neuronales, las funciones de activación desempeñan un papel fundamental al introducir no linealidad en el proceso de cálculo de una red neuronal. Estas funciones se aplican a la salida de cada neurona en una red y determinan si esa neurona debe activarse o no, es decir, si debe transmitir una señal hacia las neuronas siguientes.

Una función de activación toma como entrada la suma ponderada de las entradas de una neurona y produce una salida basada en un criterio determinado. Las funciones de activación pueden ser de dos tipos: discretas y continuas.

Las funciones de activación discretas son funciones que asignan una salida binaria, generalmente 0 o 1, según una condición de umbral. Algunos ejemplos de funciones de activación discretas son la función escalón y la función signo.

La función escalón asigna 0 si la entrada es menor que un umbral y 1 si es mayor o igual al umbral. Esta función se utiliza comúnmente en problemas de clasificación binaria, donde se necesita tomar una decisión basada en una condición de umbral.

**Ejemplo:** Imagina que estás construyendo un modelo de clasificación para distinguir si un correo electrónico es spam o no spam. Puedes utilizar la función escalón para asignar la etiqueta "spam" si la suma ponderada de las características del correo electrónico supera cierto umbral y "no spam" en caso contrario.

La función signo asigna -1 si la entrada es negativa, 0 si es cero y 1 si es positiva. Esta función también se utiliza en problemas de clasificación binaria y tiene propiedades similares a la función escalón.

**Ejemplo:** Supongamos que estás desarrollando un modelo para predecir si un tumor es maligno o benigno basado en ciertas características. Puedes aplicar la función signo para asignar el valor "-1" si el tumor es maligno y "1" si es benigno.

Por otro lado, las funciones de activación continuas son funciones que producen una salida continua y suave en lugar de una salida binaria. Algunos ejemplos de funciones de activación continuas son la función sigmoidal, la función tangente hiperbólica y la función lineal rectificada (ReLU).

La función sigmoidal, también conocida como la función de la curva S, mapea la entrada a un rango entre 0 y 1. Es especialmente útil en problemas de clasificación binaria y se utiliza comúnmente en las capas ocultas de las redes neuronales.

**Ejemplo:** Si estás desarrollando un modelo para predecir si un estudiante aprobará un examen o no, puedes aplicar la función sigmoidal para obtener una probabilidad entre 0 y 1 que represente la confianza del modelo en la predicción.

La función tangente hiperbólica (tanh) también mapea la entrada a un rango entre -1 y 1. A diferencia de la función sigmoidal, la función tangente hiperbólica puede generar valores negativos, lo que la hace adecuada para problemas de clasificación binaria y también para problemas de regresión.

**Ejemplo:** Supongamos que estás construyendo un modelo para predecir el precio de una casa en función de sus

 características. Puedes aplicar la función tangente hiperbólica para obtener una estimación del precio en un rango que abarque valores negativos y positivos.

La función lineal rectificada (ReLU) es una función que devuelve la entrada si es positiva y 0 si es negativa. Es una función simple pero ampliamente utilizada en redes neuronales, especialmente en las capas convolucionales.

**Ejemplo:** En un modelo de reconocimiento de imágenes, puedes aplicar la función ReLU para activar las neuronas que detectan características relevantes en una imagen y desactivar las neuronas que no contribuyen significativamente.

Estos son solo algunos ejemplos de las funciones de activación utilizadas en las redes neuronales. Cada función tiene un propósito y una aplicabilidad específica en el contexto de una red neuronal, y su elección depende del problema que se está abordando. En el próximo capítulo, exploraremos con más detalle las funciones de activación discretas y continuas, así como la función de activación softmax utilizada en problemas de clasificación con múltiples clases.

## 8.2. Funciones de Activación Discretas

En este capítulo, nos centraremos en las funciones de activación discretas utilizadas en las redes neuronales. Estas funciones son importantes para realizar una clasificación binaria, donde se desea asignar una salida a una de dos clases posibles. Exploraremos dos tipos de funciones de activación discretas: la función escalón y la función signo.

### 8.2.1. Función Escalón

La función escalón es una función de activación discreta que asigna un valor de salida según una condición de umbral. Si la entrada a la función es mayor o igual al umbral, la salida es igual a 1; de lo contrario, la salida es igual a 0. Esta función es útil en problemas de decisión binarios, donde se necesita tomar una decisión basada en una condición.

**Ejemplo:** Imaginemos que tenemos un modelo de red neuronal que clasifica imágenes de gatos y perros. La función escalón se puede utilizar como función de activación en la última capa de la red para asignar una salida binaria. Si la salida de la función escalón es 1, significa que la imagen se clasifica como un gato; si la salida es 0, significa que se clasifica como un perro.

```python
def step_function(x):
    threshold = 0.5
    if x >= threshold:
        return 1
    else:
        return 0

output = step_function(0.8)  # Ejemplo de clasificación de una imagen de gato
print(output)  # Salida: 1
```

En este ejemplo, si la salida del modelo es mayor o igual a 0.5, se considera que la imagen es un gato y se asigna el valor 1. De lo contrario, se clasifica como un perro y se asigna el valor 0.

La función escalón también puede tener diferentes umbrales según los requisitos del problema. El umbral se puede ajustar para adaptarse a la situación y mejorar el rendimiento de la clasificación.

## 8.2.2. Función Signo

La función signo es otra función de activación discreta que asigna un valor de salida basado en el signo de la entrada. Si la entrada es positiva o cero, la salida es igual a 1; si la entrada es negativa, la salida es igual a -1. Al igual que la función escalón, la función signo es útil en problemas de clasificación binaria.

**Ejemplo:** Supongamos que queremos clasificar comentarios en positivos y negativos. Podemos utilizar la función signo como función de activación para asignar una salida según el sentimiento del comentario. Si la salida de la función signo es 1, significa que el comentario es positivo; si la salida es -1, significa que el comentario es negativo.

```python
def sign_function(x):
    if x >= 0:
        return 1
    else:
        return -1

output = sign_function(0.3)  # Ejemplo de clasificación de un comentario positivo
print(output)  # Salida: 1
```

En este caso, si la salida del modelo es mayor o igual a 0, se considera que el comentario es positivo y se asigna el valor 1. Si la salida es negativa, se clasifica como un comentario negativo y se asigna el valor -1.

Al igual que la función escalón, la función sign

o también puede ajustarse para adaptarse a diferentes umbrales según los requisitos del problema.

**Ejemplo:** En el campo de la detección de fraudes, se pueden utilizar funciones de activación discretas para clasificar transacciones como legítimas o fraudulentas. La función signo puede ser útil en este caso. Si la salida es 1, significa que la transacción se clasifica como fraudulenta y se toman medidas adecuadas; si la salida es -1, significa que la transacción es legítima y no se requieren acciones adicionales.

Este ejemplo ilustra cómo las funciones de activación discretas pueden ser aplicadas en diferentes escenarios para la clasificación binaria.

## 8.3. Funciones de activación continuas

En este apartado, nos enfocaremos en las funciones de activación continuas más comúnmente utilizadas en las redes neuronales. Estas funciones desempeñan un papel fundamental al introducir no linealidad en el proceso de cálculo de una red neuronal. A continuación, explicaremos cada una de estas funciones y su aplicabilidad en diferentes contextos.

### 8.3.1. Función sigmoidal

La función sigmoidal es una de las funciones de activación continuas más utilizadas en las redes neuronales. Su forma característica es una curva en forma de "S" y se utiliza para mapear los valores de entrada a un rango entre 0 y 1. La fórmula general de la función sigmoidal es la siguiente:

```
f(x) = 1 / (1 + exp(-x))
```

Donde `x` es el valor de entrada. Esta función es especialmente útil en problemas de clasificación binaria, donde se desea asignar una probabilidad a cada clase.

**Ejemplo:** Supongamos que tenemos una red neuronal que clasifica imágenes de perros y gatos. Después de procesar una imagen, la red neuronal produce un valor de activación de 0.75 para la clase "perro". Aplicando la función sigmoidal a este valor, obtenemos:

```
f(0.75) = 1 / (1 + exp(-0.75)) ≈ 0.678
```

Esto indica que la red neuronal asigna una probabilidad de aproximadamente 0.678 a la clase "perro".

La función sigmoidal tiene la propiedad de producir salidas en el rango (0, 1), lo que permite interpretarlas como probabilidades. Sin embargo, esta función puede sufrir del problema de "desvanecimiento de gradientes" en redes neuronales profundas, donde los gradientes se vuelven muy pequeños durante el proceso de retropropagación. Para abordar este problema, se han propuesto otras funciones de activación, como la función tangente hiperbólica y la función lineal rectificada (ReLU).

### 8.3.2. Función tangente hiperbólica (tanh)

La función tangente hiperbólica, también conocida como tanh, es otra función de activación continua ampliamente utilizada en redes neuronales. Su forma se asemeja a la función sigmoidal, pero mapea los valores de entrada a un rango entre -1 y 1. La fórmula general de la función tanh es la siguiente:

```
f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
```

La función tanh es simétrica alrededor del origen y tiene un comportamiento no lineal en el rango de -1 a 1. Esta función es útil cuando se desea mantener la forma y la distribución de los datos en el proceso de cálculo de la red neuronal.

**Ejemplo:** Supongamos que tenemos una red neuronal que clasifica opiniones de películas como positivas o negativas. Después de procesar una opinión, la red neuronal produce un valor de activación de -0.5. Aplicando la función tanh a este valor, obtenemos:

```
f(-0.5) = (exp(-0.5) - exp(-(-0.5))) / (exp(-0.5)

 + exp(-(-0.5))) ≈ -0.462
```

Esto indica que la red neuronal asigna una puntuación de aproximadamente -0.462 a la opinión, lo que sugiere una inclinación hacia la clasificación como negativa.

La función tanh es útil en redes neuronales donde se requiere una salida simétrica y no lineal en el rango de -1 a 1.

### 8.3.3. Función lineal rectificada (ReLU)

La función lineal rectificada, también conocida como ReLU (Rectified Linear Unit), es una función de activación continua que se ha vuelto extremadamente popular en las redes neuronales. Esta función mapea los valores de entrada a través de la siguiente regla:

```
f(x) = max(0, x)
```

La función ReLU produce una salida de 0 para todos los valores de entrada negativos y conserva el valor de entrada para todos los valores positivos. En otras palabras, la función ReLU activa la neurona solo si la entrada es positiva, de lo contrario, la desactiva.

**Ejemplo:** Supongamos que tenemos una red neuronal para identificar objetos en imágenes y la salida de una neurona es 0.8. Aplicando la función ReLU a este valor, obtenemos:

```
f(0.8) = max(0, 0.8) = 0.8
```

En este caso, la función ReLU no modifica la salida ya que el valor de entrada es positivo.

La función ReLU es computacionalmente eficiente y se ha demostrado que ayuda a superar el problema de desvanecimiento de gradientes en redes neuronales profundas. Además, la función ReLU introduce esparcidad en las activaciones de las neuronas, lo que puede mejorar la capacidad de generalización del modelo.

## 8.4. Función de activación softmax

En este apartado, nos enfocaremos en la función de activación softmax, una función utilizada en las redes neuronales para asignar probabilidades a un conjunto de clases mutuamente excluyentes. Exploraremos su aplicación en problemas de clasificación con múltiples clases y entenderemos cómo se interpreta la salida de la función softmax. Además, veremos ejemplos concretos de casos en los que la función de activación softmax es esencial para el procesamiento de información en redes neuronales.

### 8.4.1. ¿Qué es la función de activación softmax?

La función de activación softmax es una función matemática que toma un vector de números reales y produce una salida que representa una distribución de probabilidad sobre las clases. Es especialmente útil cuando se trabaja con problemas de clasificación con más de dos clases, ya que asigna probabilidades a cada clase de manera que la suma de todas las probabilidades sea igual a 1.

La función softmax se define matemáticamente de la siguiente manera:

```
softmax(x_i) = exp(x_i) / sum(exp(x_j)), para todo x_i en el vector de entrada
```

Donde `exp(x_i)` representa la función exponencial de `x_i`, y la suma se realiza sobre todos los elementos del vector de entrada.

### 8.4.2. Aplicación de la función de activación softmax en problemas de clasificación

La función de activación softmax es ampliamente utilizada en problemas de clasificación con múltiples clases. Proporciona una forma de asignar probabilidades a cada clase en base a las entradas de la red neuronal. La salida de la función softmax se interpreta como la probabilidad de pertenencia de una instancia a cada una de las clases.

**Ejemplo:** Supongamos que tenemos un problema de clasificación de imágenes en el que queremos determinar si una imagen pertenece a una de las siguientes clases: perro, gato o pájaro. Después de pasar la imagen a través de la red neuronal, aplicamos la función softmax a la capa de salida para obtener las probabilidades de pertenencia a cada clase.

La salida de la función softmax podría ser algo como:
- Probabilidad de perro: 0.7
- Probabilidad de gato: 0.2
- Probabilidad de pájaro: 0.1

Esto indica que la red neuronal estima que la imagen tiene un 70% de probabilidad de ser un perro, un 20% de probabilidad de ser un gato y un 10% de probabilidad de ser un pájaro.

**Ejemplo:** Imaginemos que tenemos un conjunto de imágenes de dígitos escritos a mano (0-9) y queremos entrenar una red neuronal para clasificarlos correctamente. Utilizando la función de activación softmax en la capa de salida de la red, podemos obtener las probabilidades de pertenencia a cada dígito para una imagen dada.

Por ejemplo, si aplicamos la función softmax a la salida de la red para una imagen de un dígito "7", podríamos obtener las siguientes probabilidades:
- Probabilidad de ser "0": 0.01
- Probabilidad de ser "1": 0.05
- Probabilidad de ser "2": 0.03
-

 Probabilidad de ser "3": 0.02
- Probabilidad de ser "4": 0.01
- Probabilidad de ser "5": 0.03
- Probabilidad de ser "6": 0.05
- Probabilidad de ser "7": 0.75
- Probabilidad de ser "8": 0.02
- Probabilidad de ser "9": 0.04

En este caso, la red neuronal estima con una alta probabilidad del 75% que la imagen corresponde al dígito "7".

**Ejemplo:** Supongamos que queremos desarrollar un modelo de clasificación de sentimientos en texto, donde deseamos determinar si un determinado texto expresa emociones positivas, negativas o neutras. Utilizando la función de activación softmax en la capa de salida de la red, podemos asignar probabilidades a cada clase de sentimiento.

Por ejemplo, si aplicamos la función softmax a la salida de la red para un texto determinado, podríamos obtener las siguientes probabilidades:
- Probabilidad de sentimiento positivo: 0.6
- Probabilidad de sentimiento negativo: 0.3
- Probabilidad de sentimiento neutro: 0.1

Esto indica que el modelo estima con una probabilidad del 60% que el texto expresa un sentimiento positivo, un 30% de probabilidad de ser negativo y un 10% de probabilidad de ser neutro.

La función de activación softmax permite obtener estas probabilidades y nos ayuda a tomar decisiones basadas en la salida de la red neuronal en problemas de clasificación con múltiples clases.