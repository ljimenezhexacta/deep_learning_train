# 6. La Neurona: La Unidad Básica de una Red Neuronal
En este capítulo se profundizará en el concepto de la neurona, que es la unidad básica de una red neuronal. Aquí se explorará en detalle la estructura y el funcionamiento de una neurona artificial en una red neuronal. Se abordarán conceptos como las entradas, los pesos, las sumas ponderadas, las funciones de activación y las salidas de una neurona. Se explicará cómo se realiza el cálculo de una neurona y cómo contribuye a la predicción final del modelo.

## 6.1. Estructura y funcionamiento de una neurona artificial

Para entender mejor el funcionamiento de las redes neuronales, debemos desglosar sus componentes fundamentales y conocer cómo interactúan entre sí. Uno de estos componentes es la neurona artificial, una estructura inspirada en las neuronas biológicas de nuestro cerebro.

### Conceptos clave

#### Neurona Artificial
Una neurona artificial, también conocida como nodo o unidad, es la unidad de procesamiento básica en una red neuronal. Se encarga de recibir una o más entradas, realizar un cálculo basado en estas entradas y producir una salida.

**Ejemplo:** Imagina que estás en un equipo de debate. Cada miembro del equipo (una neurona) recibe información sobre un tema (las entradas), la pondera basándose en sus propios conocimientos y opiniones (pesos), luego presenta un argumento (salida) basándose en esa información ponderada.

#### Entradas
Las entradas a una neurona artificial son los datos que se están procesando. Podrían ser las características de un conjunto de datos, como la edad, el sexo, y el peso en un modelo de predicción de salud, o los pixeles de una imagen en un modelo de reconocimiento de imágenes.

**Ejemplo:** En una receta de cocina, las entradas serían los ingredientes que utilizamos. Cada ingrediente tiene un papel específico y contribuye de forma única al resultado final, al igual que las entradas en una neurona.

#### Pesos
Los pesos son valores numéricos que determinan la importancia relativa de cada entrada a una neurona. Estos se aprenden y se ajustan durante el proceso de entrenamiento de la red neuronal.

**Ejemplo:** Siguiendo con la analogía de la receta de cocina, los pesos serían las proporciones en las que utilizamos los ingredientes. Al ajustar las proporciones (pesos), podemos cambiar el sabor del plato (salida).

#### Suma ponderada
La suma ponderada es el resultado de multiplicar cada entrada por su peso correspondiente, y luego sumar todos estos productos. Es la combinación de las entradas a la neurona, ponderada por su importancia relativa.

**Ejemplo:** Cuando preparas un cóctel, la suma ponderada sería el resultado de mezclar diferentes licores y zumos (entradas) en determinadas proporciones (pesos). El cóctel resultante es una mezcla ponderada de sus ingredientes.

#### Función de activación
La función de activación es una función matemática aplicada a la suma ponderada en la neurona. Esta función determina la salida de la neurona. Puede ser lineal, como la identidad, o no lineal, como la función sigmoidea o la función ReLU.

**Ejemplo:** Siguiendo con la analogía del cóctel, la función de activación sería como el último toque que le das al cóctel antes de servirlo, como añadir hielo

 o una rodaja de limón. Este último toque puede cambiar la presentación y el sabor del cóctel (la salida de la neurona).

#### Salida
La salida de una neurona es el resultado de aplicar la función de activación a la suma ponderada. Esta salida puede ser enviada a otras neuronas en la red o puede representar la predicción final del modelo.

**Ejemplo:** En el cóctel, la salida sería el resultado final: el cóctel listo para beber. Esta bebida es el resultado de combinar varios ingredientes (entradas) en ciertas proporciones (pesos), mezclarlos (suma ponderada), y añadirle el toque final (función de activación).

## 6.2. Tipos de funciones de activación

En este apartado, exploraremos los diferentes tipos de funciones de activación utilizadas en las neuronas artificiales. Las funciones de activación son fundamentales en el funcionamiento de una neurona, ya que determinan su comportamiento y afectan la salida de la misma. Comprenderemos la importancia y la influencia de estas funciones en el rendimiento de una red neuronal.

### Funciones de activación en las neuronas artificiales

En una red neuronal artificial, una función de activación se aplica a la suma ponderada de las entradas para producir la salida de una neurona. Estas funciones introducen no linealidades en el modelo, permitiendo que la red neuronal aprenda y represente relaciones más complejas entre los datos de entrada y las salidas deseadas.

Existen varios tipos de funciones de activación utilizadas en las redes neuronales. A continuación, explicaremos algunos de los más comunes:

#### 1. Función de activación Sigmoide

La función de activación sigmoide es una función no lineal que transforma cualquier valor en un rango entre 0 y 1. Su forma característica es la de una curva en forma de "S". Esta función es ampliamente utilizada en redes neuronales debido a su capacidad para mapear valores a una probabilidad.

**Ejemplo:** Imagina que tienes un detector de spam en tu correo electrónico. La función de activación sigmoide se puede utilizar para calcular la probabilidad de que un correo electrónico sea spam (0 indica que no es spam y 1 indica que es spam). A medida que la función de activación se acerca a 1, la probabilidad de que el correo sea spam aumenta.

#### 2. Función de activación ReLU

La función de activación ReLU (Rectified Linear Unit) es una función lineal para valores positivos y cero para valores negativos. Es una de las funciones de activación más utilizadas en redes neuronales debido a su simplicidad y eficiencia computacional.

**Ejemplo:** Supongamos que estás construyendo un modelo para predecir el precio de las casas. La función de activación ReLU puede ayudar a manejar los valores negativos en las predicciones. Si el valor predicho es negativo, la función ReLU lo convertirá en cero, lo que indica que el precio no puede ser negativo.

#### 3. Función de activación Tanh

La función de activación Tangente Hiperbólica (Tanh) es similar a la función sigmoide, pero su rango es de -1 a 1. Esta función también es no lineal y se utiliza comúnmente en redes neuronales.

**Ejemplo:** Imagina que estás construyendo un modelo para clasificar imágenes de gatos y perros. La función de activación Tanh se puede usar para asignar valores entre -1 y 1 a las características extraídas de las imágenes. Un valor cercano a 1 indica alta probabilidad de ser un gato, mientras que un valor cercano a -1 indica alta probabilidad de ser un perro.

#### 4. Función de activación Softmax

La función de activación Softmax se utiliza comúnmente en la capa de salida de una red neuronal para problemas de clasificación multiclase. Esta función toma un vector de números reales y los normaliza en un vector de probabilidades que suman 1.

**Ejemplo:** Supongamos que estás construyendo un modelo de reconocimiento de dígitos escritos a mano. La función de activación Softmax se puede aplicar a las salidas de la red para obtener una distribución de probabilidad sobre los diferentes dígitos posibles (0 al 9). La clase con la probabilidad más alta se considera la predicción del modelo.

Estos son solo algunos ejemplos de funciones de activación utilizadas en las redes neuronales. Cada función tiene sus propias características y aplicaciones, y la elección de la función de activación adecuada depende del problema y los datos específicos.

Recuerda que estas funciones de activación introducen la no linealidad necesaria para que una red neuronal pueda aprender patrones complejos y realizar predicciones precisas. Experimentar con diferentes funciones de activación es parte del proceso de desarrollo y ajuste de una red neuronal para obtener los mejores resultados en un problema dado.

## 6.3. Entrenamiento y ajuste de pesos en una neurona

En este apartado, nos adentraremos en el proceso de entrenamiento y ajuste de pesos en una neurona de una red neuronal. El entrenamiento de una neurona es esencial para lograr un modelo de predicción preciso. Exploraremos técnicas fundamentales como el aprendizaje supervisado y el descenso del gradiente, que permiten optimizar los pesos de la neurona y mejorar su rendimiento.

### 6.3.1. Aprendizaje supervisado: Una técnica fundamental

El aprendizaje supervisado es una técnica clave en el entrenamiento de una neurona en una red neuronal. Consiste en enseñar al modelo utilizando ejemplos de entrada y salida esperada. Es decir, se proporciona un conjunto de datos etiquetados, donde cada dato tiene una entrada y una salida asociada. A partir de estos ejemplos, la neurona ajusta sus pesos para producir salidas cada vez más precisas.

**Ejemplo:** Supongamos que queremos entrenar una neurona para reconocer imágenes de gatos y perros. Se proporciona un conjunto de imágenes etiquetadas donde se indica si la imagen es de un gato o de un perro. Durante el entrenamiento, la neurona ajusta sus pesos para asociar las características de las imágenes con las etiquetas correspondientes. De esta manera, la neurona aprenderá a distinguir entre gatos y perros al recibir nuevas imágenes.

El aprendizaje supervisado es ampliamente utilizado en tareas como clasificación de texto, reconocimiento de objetos, detección de fraudes, entre otros. Proporciona un enfoque estructurado para que la neurona aprenda a partir de ejemplos con información explícita de entrada y salida.

### 6.3.2. Descenso del gradiente: Optimizando los pesos de la neurona

El descenso del gradiente es un algoritmo utilizado para ajustar los pesos de una neurona y encontrar los valores óptimos que minimizan la diferencia entre las salidas predichas y las salidas esperadas. En esencia, este algoritmo busca "descender" por la función de pérdida para encontrar el punto donde la pérdida es mínima.

**Ejemplo:** Imagina que tienes un terreno con colinas y valles, y quieres llegar al punto más bajo del terreno. El descenso del gradiente sería similar a caminar por la pendiente más pronunciada en cada paso, ajustando tu posición hasta alcanzar el valle más bajo. De esta manera, el descenso del gradiente busca encontrar los pesos de la neurona que minimicen la diferencia entre las salidas predichas y las salidas esperadas.

En el contexto del entrenamiento de una neurona, la función de pérdida mide la discrepancia entre las salidas predichas por la neurona y las salidas esperadas. El descenso del gradiente utiliza esta información para actualizar los pesos de la neurona en dirección a la minimización de la pérdida.

El descenso del gradiente se aplica iterativamente, ajustando los pesos en cada paso hasta converger hacia una solución óptima. Es un proceso fundamental en el entrenamiento de redes neuronales y juega un papel clave en la optimización del rendimiento del modelo.

El entrenamiento de una neurona es un proceso iterativo que requiere una selección cuidadosa de algoritmos y parámetros, así como un conjunto de datos de entrenamiento representativo. El objetivo final es encontrar los pesos óptimos que permitan a la neurona realizar predicciones precisas y generalizar correctamente a nuevos datos.